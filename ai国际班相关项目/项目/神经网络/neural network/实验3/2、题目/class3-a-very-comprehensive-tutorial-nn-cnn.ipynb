{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5798f0f0-ef68-4630-866c-c6c6cd459fa7",
    "_uuid": "20b6dcf907bd223863cbbe5f4995c284282b05d9"
   },
   "source": [
    "# <a>Understanding and Implementing Neural Networks from scratch</a>\n",
    "#    从头开始理解和实现神经网络\n",
    "\n",
    "<br>\n",
    "\n",
    "In this kernel, I have explained the intution about neural networks and how to implement neural networks from scratch in python. \n",
    "# 在这个内核中，我解释了关于神经网络的直觉以及如何在python中从头实现神经网络。\n",
    "\n",
    "## Contents  \n",
    "#  内容\n",
    "<br>\n",
    "\n",
    "**<a><i> 1. What are Neural Networks</i></a> **    # 什么是神经网络\n",
    "\n",
    "**<a><i> 2. Implement a Neural Network - Binary classification</i></a>**   # 实现一个神经网络-二进制分类\n",
    "\n",
    "**<a><i> 3. Implement a Neural Network - Multiclass classification</i></a>**   #实现一个神经网络-多级分类\n",
    "\n",
    "**<a><i> 4. What are Deep Neural Networks</i></a> **     #什么是深度神经网络\n",
    "\n",
    "**<a><i> 5. Convolutional Neural Networks Implementation</i></a>**   # 卷积神经网络实现\n",
    "\n",
    "![](https://www.pangeanic.com/wp-content/uploads/sites/2/2017/07/neural-network-graph-624x492.jpg)\n",
    "\n",
    "I would like to thank Andrew NG and deeplearning.ai course for their excellent material\n",
    "# 我要感谢Andrew NG和deeplearning。人工智能课程为他们提供了优秀的材料\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## <a>1. What are Neural Networks </a>\n",
    "\n",
    "Neural networks are a type of machine learning models which are designed to operate similar to biological neurons and human nervous system. These models are used to recognize complex patterns and relationships that exists within a labelled dataset. They have following properties:\n",
    "# 神经网络是一种类似于生物神经元和人类神经系统的机器学习模型。这些模型用于识别标记数据集中存在的复杂模式和关系。它们有以下特性:\n",
    "\n",
    "1. The core architecture of a Neural Network model is comprised of a large number of simple processing nodes called Neurons which are interconnected and organized in different layers.\n",
    "#  神经网络模型的核心结构是由大量被称为神经元的简单处理节点组成的，这些节点在不同的层中相互连接和组织。 \n",
    "\n",
    "2. An individual node in a layer is connected to several other nodes in the previous and the next layer. The inputs form one layer are received and processed to generate the output which is passed to the next layer.\n",
    "#  层中的单个节点连接到上一层和下一层中的其他几个节点。接收并处理来自一层的输入，以生成传递到下一层的输出。\n",
    "\n",
    "3. The first layer of this architecture is often named as input layer which accepts the inputs, the last layer is named as the output layer which produces the output and every other layer between input and output layer is named is hidden layers.\n",
    "#  这种体系结构的第一层通常被称为输入层，它接受输入，最后一层被称为输出层，它产生输出，输入和输出层之间的每一层都被称为隐藏层。 \n",
    "\n",
    "### Key concepts in a Neural Network \n",
    "#   神经网络的关键概念\n",
    "\n",
    "#### A. Neuron:\n",
    "#       神经元\n",
    "\n",
    "A Neuron is a single processing unit of a Neural Network which are connected to different other neurons in the network. These connections repersents inputs and ouputs from a neuron. To each of its connections, the neuron assigns a “weight” (W) which signifies the importance the input and adds a bias (b) term.\n",
    "# 神经元是神经网络的单一处理单元，它与网络中不同的神经元相连。这些连接代替了神经元的输入和输出。神经元给每个连接分配一个“权值”(W)，表示输入的重要性，并添加一个偏差项(b)。 \n",
    "\n",
    "#### B. Activation Functions \n",
    "#       激活函数\n",
    "\n",
    "The activation functions are used to apply non-linear transformation on input to map it to output. The aim of activation functions is to predict the right class of the target variable based on the input combination of variables. Some of the popular activation functions are Relu, Sigmoid, and TanH. \n",
    "# 激活函数用于对输入应用非线性变换，将其映射到输出。激活函数的目的是根据变量的输入组合来预测目标变量的正确类别。常用的激活函数有Relu、Sigmoid和TanH。\n",
    "\n",
    "#### C. Forward Propagation \n",
    "#       正向传播\n",
    "Neural Network model goes through the process called forward propagation in which it passes the computed activation outputs in the forward direction.\n",
    "# 神经网络模型通过一个称为正向传播的过程，在这个过程中，它将计算出的激活输出传递给正向。 \n",
    "\n",
    "Z = W*X + b   \n",
    "A = g(Z) \n",
    "\n",
    "- g is the activation function                # g是激活函数\n",
    "- A is the activation using the input         # A是使用输入的激活\n",
    "- W is the weight associated with the input   # W是与输入相关的权值\n",
    "- B is the bias associated with the node      # B是与节点相关的偏差\n",
    " \n",
    "#### D. Error Computation: \n",
    "#        误差计算\n",
    "\n",
    "The neural network learns by improving the values of weights and bias. The model computes the error in the predicted output in the final layer which is then used to make small adjustments the weights and bias. The adjustments are made such that the total error is minimized. Loss function measures the error in the final layer and cost function measures the total error of the network. \n",
    "# 神经网络通过改进权值和偏差值进行学习。该模型计算最后一层预测输出的误差，然后对权重和偏差进行小的调整。这些调整使得总误差最小化。损耗函数测量最后一层的误差，成本函数测量网络的总误差。\n",
    "\n",
    "Loss = Actual_Value - Predicted_Value   \n",
    "\n",
    "Cost = Summation (Loss)   \n",
    "\n",
    "#### E. Backward Propagation:\n",
    "#        反向传播 \n",
    "\n",
    "Neural Network model undergoes the process called backpropagation in which the error is passed to backward layers so that those layers can also improve the associated values of weights and bias. It uses the algorithm called Gradient Descent in which the error is minimized and optimal values of weights and bias are obtained. This weights and bias adjustment is done by computing the derivative of error, derivative of weights, bias and subtracting them from the original values. \n",
    "# 神经网络模型经历了一个称为反向传播的过程，在这个过程中误差被传递到反向层，这样那些层也可以改进相关的权值和偏差。该算法采用梯度下降法，使误差最小化，获得最优的权值和偏差。这种权值和偏差调整是通过计算误差的导数、权值的导数、偏差并从原始值中减去它们来实现的。\n",
    "<br>\n",
    "\n",
    "## <a> 2. Implement a Neural Network - Binary Classification</a>\n",
    "#          实现一个神经网络-二进制分类  \n",
    "\n",
    "Lets implement a basic neural network in python for binary classification which is used to classify if a given image is 0 or 1.  \n",
    "# 让我们实现一个基本的神经网络在python的二进制分类，这是用来分类，如果一个给定的图像是0或1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "abf6aa77-17cd-4b4a-bacf-903838fe813e",
    "_uuid": "735f9f4ab4893a28cc6a28f81a49a182fd02747d",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e249a1aa-a3df-4342-97ea-48a380d642a0",
    "_uuid": "3fa615d978d15f0430a3adf0bbe2b7ccc8205f5f"
   },
   "source": [
    "### 2.1 Dataset Preparation\n",
    "#       数据集的准备\n",
    "\n",
    "First step is to load and prepare the dataset\n",
    "# 第一步是加载和准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "34aa89c5-b5b7-4b34-9f16-0af4a4046cdf",
    "_uuid": "670c2f126fa04d98426502d720a6288e5b0cb5df",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/H/Desktop/AI/nurual network/实验3/1、数据/train.csv\")\n",
    "test = pd.read_csv(\"C:/Users/H/Desktop/AI/nurual network/实验3/1、数据/test.csv\")\n",
    "\n",
    "# include only the rows having label = 0 or 1 (binary classification)\n",
    "# 只包含label = 0或1的行(二进制分类)\n",
    "X = train[train['label'].isin([0, 1])]\n",
    "\n",
    "# target variable\n",
    "# 目标变量\n",
    "Y = train[train['label'].isin([0, 1])]['label']\n",
    "\n",
    "# remove the label from X\n",
    "# 从X中移除标签\n",
    "X = X.drop(['label'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n0       0       0       0       0       0       0       0       0       0   \n1       0       0       0       0       0       0       0       0       0   \n2       0       0       0       0       0       0       0       0       0   \n4       0       0       0       0       0       0       0       0       0   \n5       0       0       0       0       0       0       0       0       0   \n\n   pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n5       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n4         0         0         0         0  \n5         0         0         0         0  \n\n[5 rows x 784 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>pixel9</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 784 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 36
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "378fd937-d372-4eab-bcfd-46bf5817ed01",
    "_uuid": "37ea4b7a6bbedd53b00dec3cfefad523b44d68fb"
   },
   "source": [
    "### 2.2 Implementing a Activation Function \n",
    "#        实现激活函数\n",
    "\n",
    "We will use sigmoid activation function because it outputs the values between 0 and 1 so its a good choice for a binary classification problem\n",
    "# 我们将使用sigmoid激活函数，因为它输出0到1之间的值，所以它是一个很好的二进制分类问题的选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "19fc3ff1-f6ac-447d-98ea-4ff4d4bf4669",
    "_uuid": "13f0338a967a4cb87503821f044c09552d2e1640",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# implementing a sigmoid activation function\n",
    "# 实现一个sigmoid激活函数\n",
    "def sigmoid(z):\n",
    "    #########################需要填写，写出sigmoid的计算公式，并赋值给s，sigmoid=1/1+exp(x)\n",
    "    s = 1.0/(1 + np.exp(-z))\n",
    "    #########################需要填写，写出sigmoid的计算公式，并赋值给s，sigmoid=1/1+exp(x)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ee1e1ebe-4298-40cd-9044-d8dc4327aa6c",
    "_uuid": "3c58c24a7feb9789a5bd84e8c9cc5068eebbee04"
   },
   "source": [
    "### 2.3 Define Neural Network Architecture\n",
    "#        定义神经网络体系结构\n",
    "\n",
    "Create a model with three layers - Input, Hidden, Output. \n",
    "# 创建一个有三层的模型——输入，隐藏，输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "a4c98dcc-6a34-4e5d-99d7-7e26ec12ce21",
    "_uuid": "deab1ad6439f304b669f70da3ca1f70e13f027f6",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def network_architecture(X, Y):\n",
    "    # nodes in input layer\n",
    "    # 输入层节点\n",
    "    \n",
    "    ##########################需要填写，计算每层的个数，输入层个数赋值给n_x,输出层赋值给n_y\n",
    "    n_x = X.shape[0]\n",
    "    \n",
    "    # nodes in hidden layer\n",
    "    # 隐藏层节点\n",
    "    n_h = 10  \n",
    "    \n",
    "    # nodes in output layer\n",
    "    # 输出层节点\n",
    "    n_y = Y.shape[0]\n",
    "     ##########################需要填写，计算每层的个数，输入层个数赋值给n_x,输出层赋值给n_y\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc75a514-66e5-4479-9613-7fdc4572271d",
    "_uuid": "c91f07b7c28808b74babea3d013fbfdd6ea3ba91"
   },
   "source": [
    "### 2.4 Define Neural Network Parameters \n",
    "#       定义神经网络参数\n",
    "Neural Network parameters are weights and bias which we need to initialze with zero values. The first layer only contains inputs so there are no weights and bias, but the hidden layer and the output layer have a weight and bias term. (W1, b1 and W2, b2)\n",
    "# 神经网络参数是权重和偏差，我们需要用零值初始化。第一层只包含输入，所以没有权值和偏差，但是隐含层和输出层有一个权值和偏差项。(W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "cef24778-cd2c-4412-b017-7f4d9b34f34a",
    "_uuid": "c13e2d5cd4d04c05c878f7ccab1677b290847be4",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def define_network_parameters(n_x, n_h, n_y):\n",
    "    W1 = np.random.randn(n_h,n_x) * 0.01      # random initialization\n",
    "    b1 = np.zeros((n_h, 1))                   # zero initialization\n",
    "    \n",
    "    ##############################需要填写，请为输出层的W2和b2赋值\n",
    "    W2 = np.random.randn(n_y,n_h) * 0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ##############################需要填写，请为输出层的W2和b2赋值\n",
    "    \n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd1fd5e9-aae9-4996-8137-6f577147b90c",
    "_uuid": "9664d29beedd6c5a7c27b10201992a3c2f810c56"
   },
   "source": [
    "### 2.5 Implement Forward Propagation\n",
    "#       实现向前传播\n",
    "\n",
    "The hidden layer and output layer will compute the activations using sigmoid activation function and will pass it in the forward direction. While computing this activation, the input is multiplied with weight and added with bias before passing it to the function.\n",
    "# 隐含层和输出层使用sigmoid激活函数计算激活量，并向前传递。在计算这个激活时，输入与权重相乘，并在传递给函数之前加上偏差。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_cell_guid": "2fd5cfe1-dbe2-4580-b541-1ed5cf139166",
    "_uuid": "ee7d22c45eeb9768b2722c36ecf9f73ff9ef2db9",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, params):\n",
    "    Z1 = np.dot(params['W1'], X)+params['b1']\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    ################################需要填写，请计算Z2和A2\n",
    "    Z2 = np.dot(params['W2'], A1)+params['b2']\n",
    "    A2 = sigmoid(Z2)\n",
    "    ################################需要填写，请计算Z2和A2\n",
    "    \n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da5fac38-7256-4fc3-bcfc-b2358b8bff31",
    "_uuid": "162413bf075111e6355ee9d5f16d0ca78a2f33a9"
   },
   "source": [
    "### 2.6 Compute the Network Error \n",
    "#       计算网络误差\n",
    "\n",
    "To compute the cost, one straight forward approach is to compute the absolute error among prediction and actual value. But a better loss function is the log loss function which is defines as : \n",
    "# 计算成本的一种直接方法是计算预测值与实际值之间的绝对误差。但更好的损失函数是对数损失函数，定义为:\n",
    "  -Summ ( Log (Pred) * Actual + Log (1 - Pred ) * Actual ) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "155f019d-3e33-4cdd-9722-4d0a7121276f",
    "_uuid": "08d7b796a3dbdcb1530053238037803ac76362c3",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_error(Predicted, Actual):                   #计算误差函数，可以选择不同的误差函数\n",
    "    logprobs = np.multiply(np.log(Predicted), Actual)+ np.multiply(np.log(1-Predicted), 1-Actual)\n",
    "    cost = -np.sum(logprobs) / Actual.shape[1] \n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c4ef52b-55ac-4ef2-a5fd-ee164db8dcdc",
    "_uuid": "58d61c5257c8f568d439a605fc311f483f09f1e2"
   },
   "source": [
    "### 2.7 Implement Backward Propagation\n",
    "#       实现反向传播\n",
    "In backward propagation function, the error is passed backward to previous layers and the derivatives of weights and bias are computed. The weights and bias are then updated using the derivatives. \n",
    "# 在反向传播函数中，将误差反向传递到前一层，计算权值和偏差的导数。然后使用导数更新权重和偏差。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_cell_guid": "8796bbb2-8fbf-4740-9687-c3ec2c38a27f",
    "_uuid": "195d2b62d4655b7ecff22e4bc70be15256aadf1b",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def backward_propagation(params, activations, X, Y):                ##########反向传播算法，计算导数############\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # output layer\n",
    "    # 输出层\n",
    "    dZ2 = activations['A2'] - Y                      # compute the error derivative  计算误差导数\n",
    "    dW2 = np.dot(dZ2, activations['A1'].T) / m       # compute the weight derivative 计算权重导数\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)/m       # compute the bias derivative   计算偏导\n",
    "    \n",
    "    # hidden layer\n",
    "    # 隐藏层\n",
    "    dZ1 = np.dot(params['W2'].T, dZ2)*(1-np.power(activations['A1'], 2))\n",
    "    dW1 = np.dot(dZ1, X.T)/m\n",
    "    db1 = np.sum(dZ1, axis=1,keepdims=True)/m\n",
    "    \n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "def update_parameters(params, derivatives, alpha = 1.2):           ##########反向传播算法，更新参数############\n",
    "    # alpha is the model's learning rate\n",
    "    # 是模型的学习率\n",
    "    \n",
    "    params['W1'] = params['W1'] - alpha * derivatives['dW1']\n",
    "    params['b1'] = params['b1'] - alpha * derivatives['db1']\n",
    "    \n",
    "    ####################需要填写，请计算W2和b2更新之后的值\n",
    "    params['W2'] = params['W2'] - alpha * derivatives['dW2']\n",
    "    params['b2'] = params['b2'] - alpha * derivatives['db2']\n",
    "    ####################需要填写，请计算W2和b2更新之后的值\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "935845e8-c35f-43ae-8f28-65eee43fc428",
    "_uuid": "1724d52016b387a2f974e6c6fc27c2a17a96166b"
   },
   "source": [
    "### 2.8 Compile and Train the Model\n",
    "#       编译和培训模型\n",
    "\n",
    "Create a function which compiles all the key functions and creates a neural network model. \n",
    "# 创建一个编译所有关键函数并创建一个神经网络模型的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_cell_guid": "84c99459-755e-4795-a52f-bb8cf59299a4",
    "_uuid": "011debd87ff12ce75d29dac9383d74db9c64a35a",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def neural_network(X, Y, n_h, num_iterations=100):                      ############编译网络\n",
    "    n_x = network_architecture(X, Y)[0]\n",
    "    n_y = network_architecture(X, Y)[2]\n",
    "    \n",
    "    params = define_network_parameters(n_x, n_h, n_y)                  ############初始化参数  \n",
    "    for i in range(0, num_iterations):\n",
    "        results = forward_propagation(X, params)                       ############前向传播 \n",
    "        error = compute_error(results['A2'], Y)\n",
    "        derivatives = backward_propagation(params, results, X, Y)      ############反向传播\n",
    "        params = update_parameters(params, derivatives)                ############更新参数\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_cell_guid": "fb2fa7af-e787-4ca1-9079-f6dfb6723390",
    "_uuid": "2d983cbb3c77270b2ee66d9148315cb24888f292",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "D:\\aconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n  \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "y = Y.values.reshape(1, Y.size)\n",
    "x = X.T.as_matrix()\n",
    "model = neural_network(x, y, n_h = 10, num_iterations = 10)            ############生成一个新的网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6dccfd5a-8850-41a0-b406-47460f5bd2ed",
    "_uuid": "dc33c60645cdaba20c5a420ab6bc7e5933b6e88b"
   },
   "source": [
    "### 2.9 Predictions \n",
    "#       预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "59eed767-201d-4138-a77e-2ce8eaad83df",
    "_uuid": "df27eb0a992e1d7646c2ef5f5c7ca928bca6155e",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.95650605 0.30714376 0.95650605 ... 0.95650605 0.30714376 0.95650605]\nAccuracy: 78%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def predict(parameters, X):\n",
    "    results = forward_propagation(X, parameters)\n",
    "    print (results['A2'][0])\n",
    "    predictions = np.around(results['A2'])    \n",
    "    return predictions\n",
    "\n",
    "predictions = predict(model, x)\n",
    "print ('Accuracy: %d' % float((np.dot(y,predictions.T) + np.dot(1-y,1-predictions.T))/float(y.size)*100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}